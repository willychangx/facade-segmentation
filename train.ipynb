{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/willychangx/facade-segmentation/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcMC5I3as16q"
      },
      "source": [
        "# Download folders. You don't need to change anything, you'll be downloading from my Google Drive.\n",
        "\n",
        "%%capture\n",
        "\n",
        "!gdown https://drive.google.com/uc?id=1Knr0YRtWKf06a9tCHh1HTcgEA8AQy3Cl\n",
        "!unzip /content/segmentation_predict.zip -d /content\n",
        "%rm -rf /content/segmentation_predict.zip\n",
        "\n",
        "%cd /content/segmentation_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLxmdtj9z0Xw"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rRDmq_du2Qt"
      },
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import png\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import random\n",
        "from PIL import Image\n",
        "from colormap.colors import Color, hex2rgb\n",
        "from sklearn.metrics import average_precision_score as ap_score\n",
        "from torch.utils.data import ConcatDataset, DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "from dataset import FacadeDataset\n",
        "\n",
        "N_CLASS=5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuZYt4yBrHt-"
      },
      "source": [
        "# images are 256x256\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "params = {\n",
        "    'batch_size': 10,\n",
        "    'loss_function': nn.CrossEntropyLoss(),\n",
        "    'learning_rate': 1e-3,\n",
        "    'weight_decay': 1e-5,\n",
        "    'epochs': 32,\n",
        "    'percent': 0.20,\n",
        "    'fold': 5\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akodRK96u_tU"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.n_class = N_CLASS\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1), \n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "        )\n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "        )\n",
        "        self.layer7 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "        )\n",
        "        self.layer8 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
        "        )\n",
        "        self.layer9 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=64, out_channels=N_CLASS, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        down_pooling = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        x1 = self.layer1(x) # conv 3x3, ReLU\n",
        "        x = down_pooling(x1) # max pool 2x2\n",
        "        x2 = self.layer2(x) # conv 3x3, ReLU\n",
        "        x = down_pooling(x2) # max pool 2x2\n",
        "        x3 = self.layer3(x) # conv 3x3, ReLU\n",
        "        x = down_pooling(x3) # max pool 2x2\n",
        "        x4 = self.layer4(x) # conv 3x3, ReLU\n",
        "        x = down_pooling(x4) # max pool 2x2\n",
        "        x = self.layer5(x) # conv 3x3, ReLU, up-conv 2x2\n",
        "        x = torch.cat([x4, x], dim=1)\n",
        "        x = self.layer6(x) # conv 3x3, ReLU, up-conv 2x2\n",
        "        x = torch.cat([x3, x], dim=1)\n",
        "        x = self.layer7(x) # conv 3x3, ReLU, up-conv 2x2\n",
        "        x = torch.cat([x2, x], dim=1)\n",
        "        x = self.layer8(x) # conv 3x3, ReLU, up-conv 2x2\n",
        "        x = torch.cat([x1, x], dim=1)\n",
        "        x = self.layer9(x) # conv 3x3, ReLU, conv 1x1\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7TgxpVpvres"
      },
      "source": [
        "def save_label(label, path):\n",
        "    '''\n",
        "    Function for ploting labels.\n",
        "    '''\n",
        "    colormap = [\n",
        "        '#000000',\n",
        "        '#0080FF',\n",
        "        '#80FF80',\n",
        "        '#FF8000',\n",
        "        '#FF0000',\n",
        "    ]\n",
        "    assert(np.max(label)<len(colormap))\n",
        "    colors = [hex2rgb(color, normalise=False) for color in colormap]\n",
        "    w = png.Writer(label.shape[1], label.shape[0], palette=colors, bitdepth=4)\n",
        "    with open(path, 'wb') as f:\n",
        "        w.write(f, label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_vqo3uOvvz9"
      },
      "source": [
        "def train(trainloader, net, criterion, optimizer, device, epoch):\n",
        "    '''\n",
        "    Function for training.\n",
        "    '''\n",
        "    start = time.time()\n",
        "    running_loss = 0.0\n",
        "    running_loss_list = []\n",
        "    net = net.train()\n",
        "    for images, labels in tqdm(trainloader, disable=True):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = net(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss = loss.item()\n",
        "        running_loss_list.append(running_loss)\n",
        "    end = time.time()\n",
        "    print('[epoch %d] loss: %.3f elapsed time %.3f' %\n",
        "          (epoch, running_loss, end-start))\n",
        "    print('average training loss: %.3f' % (np.mean(running_loss_list)))\n",
        "    return np.mean(running_loss_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNH-8uJVv0Yd"
      },
      "source": [
        "def test(testloader, net, criterion, device):\n",
        "    '''\n",
        "    Function for testing.\n",
        "    '''\n",
        "    losses = 0.\n",
        "    cnt = 0\n",
        "    with torch.no_grad():\n",
        "        net = net.eval()\n",
        "        for images, labels in tqdm(testloader, disable=True):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            output = net(images)\n",
        "            loss = criterion(output, labels)\n",
        "            losses += loss.item()\n",
        "            cnt += 1\n",
        "    print('average validation loss: %.3f' % (losses / cnt))\n",
        "    return (losses/cnt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnnDzDHVv4XN"
      },
      "source": [
        "def cal_AP(testloader, net, criterion, device):\n",
        "    '''\n",
        "    Calculate Average Precision\n",
        "    '''\n",
        "    losses = 0.\n",
        "    cnt = 0\n",
        "    with torch.no_grad():\n",
        "        net = net.eval()\n",
        "        preds = [[] for _ in range(5)]\n",
        "        heatmaps = [[] for _ in range(5)]\n",
        "        for images, labels in tqdm(testloader, disable=True):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            output = net(images).cpu().numpy()\n",
        "            for c in range(5):\n",
        "                preds[c].append(output[:, c].reshape(-1))\n",
        "                heatmaps[c].append(labels[:, c].cpu().numpy().reshape(-1))\n",
        "\n",
        "        aps = []\n",
        "        for c in range(5):\n",
        "            preds[c] = np.concatenate(preds[c])\n",
        "            heatmaps[c] = np.concatenate(heatmaps[c])\n",
        "            if heatmaps[c].max() == 0:\n",
        "                ap = float('nan')\n",
        "            else:\n",
        "                ap = ap_score(heatmaps[c], preds[c])\n",
        "                aps.append(ap)\n",
        "            print(\"AP = {}\".format(ap))\n",
        "\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7c2L0vPv7Mn"
      },
      "source": [
        "def get_result(testloader, net, device, folder='output_train'):\n",
        "    result = []\n",
        "    cnt = 1\n",
        "    with torch.no_grad():\n",
        "        net = net.eval()\n",
        "        cnt = 0\n",
        "        for images, labels in tqdm(testloader, disable=True):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            output = net(images)[0].cpu().numpy()\n",
        "            c, h, w = output.shape\n",
        "            assert(c == N_CLASS)\n",
        "            y = np.zeros((h,w)).astype('uint8')\n",
        "            for i in range(N_CLASS):\n",
        "                mask = output[i]>0.5\n",
        "                y[mask] = i\n",
        "            # gt = labels.cpu().data.numpy().squeeze(0).astype('uint8')\n",
        "            gt = labels.cpu().data.numpy().astype('uint8')\n",
        "            save_label(y, './{}/y{}.png'.format(folder, cnt))\n",
        "            save_label(gt, './{}/gt{}.png'.format(folder, cnt))\n",
        "            plt.imsave(\n",
        "                './{}/x{}.png'.format(folder, cnt),\n",
        "                ((images[0].cpu().data.numpy()+1)*128).astype(np.uint8).transpose(1,2,0))\n",
        "\n",
        "            cnt += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOrUtgVhv9R2"
      },
      "source": [
        "def main():\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    test_data = FacadeDataset(flag='test_dev', data_range=(0,114), onehot=False)\n",
        "    test_loader = DataLoader(test_data, batch_size=10)\n",
        "    ap_data = FacadeDataset(flag='test_dev', data_range=(0,114), onehot=True)\n",
        "    ap_loader = DataLoader(ap_data, batch_size=10)\n",
        "    uh_data = FacadeDataset(flag='uh_test', dataDir='./uh_test/', data_range=(0,1), onehot=False)\n",
        "    uh_loader = DataLoader(uh_data, batch_size=1)\n",
        "\n",
        "    name = 'starter_net'\n",
        "    \n",
        "    train_loss_list = []\n",
        "    val_loss_list = []\n",
        "\n",
        "    indexes = [i for i in range(906)]\n",
        "    random.shuffle(indexes)\n",
        "    folds = [indexes[i::int(1/params['percent'])] for i in range(int(1/params['percent']))]\n",
        "\n",
        "    print('\\nStart training')\n",
        "    for fold in range(params['fold']):\n",
        "      print(f'\\nFold {fold+1}')\n",
        "      interim = folds[:]\n",
        "\n",
        "      evaluation_data_list = interim.pop(fold)\n",
        "\n",
        "      train_data_list = []\n",
        "      for index in range(params['fold']-1):\n",
        "        train_data_list += interim[index]\n",
        "      \n",
        "      train_data = FacadeDataset(flag='train', data_range=train_data_list, onehot=False)\n",
        "      train_loader = DataLoader(train_data, batch_size=params['batch_size'])\n",
        "      \n",
        "      evaluation_data = FacadeDataset(flag='train', data_range=evaluation_data_list, onehot=False)\n",
        "      evaluation_loader = DataLoader(evaluation_data, batch_size=params['batch_size'])\n",
        "\n",
        "      evaluation_data_AP = FacadeDataset(flag='train', data_range=evaluation_data_list, onehot=True)\n",
        "      evaluation_loader_AP = DataLoader(evaluation_data_AP, batch_size=params['batch_size'])\n",
        "\n",
        "      net = Net().to(device)\n",
        "\n",
        "      criterion = params['loss_function']\n",
        "      optimizer = optim.Adam(net.parameters(), params['learning_rate'], weight_decay=params['weight_decay'])\n",
        "\n",
        "      epoch_train_loss_list = []\n",
        "      epoch_val_loss_list = []\n",
        "      for epoch in range(params['epochs']): \n",
        "          print('-----------------Epoch = %d-----------------' % (epoch+1))\n",
        "          train_loss = train(train_loader, net, criterion, optimizer, device, epoch+1)\n",
        "          epoch_train_loss_list.append(train_loss)\n",
        "          val_loss = test(evaluation_loader, net, criterion, device)\n",
        "          epoch_val_loss_list.append(val_loss)\n",
        "          cal_AP(evaluation_loader_AP, net, criterion, device)\n",
        "\n",
        "      plt.plot(epoch_train_loss_list, color='blue', label=\"train\")\n",
        "      plt.plot(epoch_val_loss_list, color='olive', label=\"test\")\n",
        "      plt.ylabel('loss')\n",
        "      plt.xlabel('epoch')\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "      train_loss_list.append(np.mean(epoch_train_loss_list))\n",
        "      val_loss_list.append(np.mean(epoch_val_loss_list))\n",
        "\n",
        "    print('\\nFinished Training')\n",
        "    print(f'\\nAverage Testing Loss: {np.mean(train_loss_list)}')\n",
        "    print(f'\\nAverage Validation Loss: {np.mean(val_loss_list)}')\n",
        "\n",
        "    print('\\nTesting on test set')\n",
        "    test(test_loader, net, criterion, device)\n",
        "    print('\\nGenerating Unlabeled Result')\n",
        "    result = get_result(test_loader, net, device, folder='output_test')\n",
        "    print('\\nPredicting on UH building image')\n",
        "    res = get_result(uh_loader, net, device, folder='output_train')\n",
        "\n",
        "    torch.save(net.state_dict(), './models/model_{}.pth'.format(name)) \n",
        "\n",
        "    cal_AP(ap_loader, net, criterion, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fu5tQGIv_k2"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2Y9XuO0nUn4"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!zip -r ../segmentation_output.zip /content/segmentation/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}